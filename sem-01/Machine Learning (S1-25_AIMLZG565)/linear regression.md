# ğŸ“˜ Linear Regression â€“ Complete Notes

This document covers **Linear Regression fundamentals**, including assumptions, cost function, optimization, and error metrics (MSE, RMSE, MAE).  
It is structured step by step with explanations and examples.

---

## ğŸ”¹ 1. Assumptions of Linear Regression

Before applying Linear Regression, certain assumptions must hold:

### 1. Linearity
- Relationship between predictors (X) and target (Y) must be linear.  
- *Example*: Study hours vs exam score often increases linearly.

### 2. Independence of Errors
- Residuals should be independent of each other.  
- *Example*: In stock prices, todayâ€™s error should not depend on yesterdayâ€™s error.

### 3. Homoscedasticity (Constant Variance of Errors)
- Spread of residuals should be roughly constant across all levels of X.  
- *Example*: Predicting house prices should not have tiny errors for cheap houses and huge errors for expensive houses.

### 4. Normality of Residuals
- Residuals should follow a normal distribution.  
- Important for hypothesis testing and confidence intervals.  
- *Example*: Exam score prediction errors cluster around 0.

### 5. No Multicollinearity
- Independent variables should not be highly correlated with each other.  
- *Example*: House size and number of bedrooms are strongly correlated â†’ violates this assumption.

### 6. Mean of Residuals = 0
- On average, positive and negative errors should cancel out.  
- *Example*: If salary predictions are always underestimated by 1000 AED, mean residual â‰  0.

### 7. No Endogeneity
- Predictors should not correlate with residuals.  
- *Example*: Omitting "innate ability" in income prediction biases coefficients.

### 8. Fixed Independent Variables
- Predictors should be measured without error.  
- *Example*: Measuring height with a faulty tape violates this assumption.

---

## ğŸ”¹ 2. Cost Function in Linear Regression

The **Cost Function** measures how far predictions are from actual values.  
The most common is **Mean Squared Error (MSE)**:

<img src="https://latex.codecogs.com/svg.latex?J(\beta)=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2" />

- Lower cost = better fit.  
- The goal of regression is to minimize this cost.

---

## ğŸ”¹ 3. Global Minimum

- The cost function is typically **U-shaped (convex)**.  
- The **global minimum** is the lowest point where the cost is smallest.  
- At this point, coefficients (Î²) represent the best-fit regression line.

---

## ğŸ”¹ 4. Gradient Descent Algorithm

**Gradient Descent** is an optimization method to reach the global minimum.

### Formula

<img src="https://latex.codecogs.com/svg.latex?\beta_j=\beta_j-\alpha\frac{\partial%20J}{\partial%20\beta_j}" />

Where:
- Î± = learning rate  
- âˆ‚J/âˆ‚Î²â±¼ = gradient (slope)

### Steps
1. Start with random coefficients.  
2. Compute gradient of cost function.  
3. Update coefficients in the opposite direction.  
4. Repeat until convergence.

---

## ğŸ”¹ 5. Learning Rate (Î±)

- Controls step size in gradient descent.  

### Cases
- **Too Small (Î± â†“)** â†’ Converges slowly.  
- **Too Large (Î± â†‘)** â†’ Overshoots, may never converge.  
- **Just Right** â†’ Efficient convergence to global minimum.

*Example*: Walking down a valley:  
- Small steps â†’ reach bottom very slowly.  
- Large jumps â†’ risk bouncing around, never settling.  
- Balanced steps â†’ reach bottom smoothly.

---

## ğŸ”¹ 6. Error Metrics

### 1. Mean Squared Error (MSE)

<img src="https://latex.codecogs.com/svg.latex?MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2" />

- Penalizes large errors heavily.  
- Units are squared â†’ not intuitive.  
- Very sensitive to outliers.

---

### 2. Root Mean Squared Error (RMSE)

<img src="https://latex.codecogs.com/svg.latex?RMSE=\sqrt{MSE}" />

- Same unit as target variable (interpretable).  
- Still punishes large errors.  
- Popular in real-world regression problems.

---

### 3. Mean Absolute Error (MAE)

<img src="https://latex.codecogs.com/svg.latex?MAE=\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y}_i|" />

- Average of absolute errors.  
- A more **robust** metric when data has outliers.  
- Easy to interpret as â€œaverage error.â€

---

## ğŸ”¹ 7. Advantages & Disadvantages

| Metric | Advantages | Disadvantages |
|--------|------------|---------------|
| **MSE**  | Smooth, good for optimization, punishes large errors | Units squared, sensitive to outliers |
| **RMSE** | Same units as target, interpretable, punishes large errors | Sensitive to outliers |
| **MAE**  | Robust to outliers, simple to explain | Doesnâ€™t emphasize large errors, less smooth for optimization |

---

## ğŸ”¹ 8. Which One to Use?

- **MSE** â†’ When **large mistakes are very costly**.  
  *Example*: Predicting aircraft engine failure time (catastrophic if wrong).  

- **RMSE** â†’ When you need **interpretability** and want to penalize big mistakes more.  
  *Example*: Predicting house prices (e.g., "off by 20,000 AED on average").  

- **MAE** â†’ When **outliers exist** and you want a **robust average miss**.  
  *Example*: Predicting taxi fares (one luxury trip shouldnâ€™t dominate).  

---

## ğŸ”¹ 9. Key Takeaways

- **Assumptions** ensure regression is valid.  
- **Cost Function (MSE)** measures fit.  
- **Gradient Descent** finds the best coefficients.  
- **Learning Rate** controls convergence speed.  
- **MSE, RMSE, MAE** are core error metrics:  
  - MSE â†’ optimization, punishes large errors.  
  - RMSE â†’ interpretable, balances penalties.  
  - MAE â†’ robust, simple, less sensitive to outliers.  

# ğŸ“˜ Linear Regression & Regularization in Machine Learning

---

## âš™ï¸ Cost Function (Mean Squared Error - MSE)
The cost function measures how well the modelâ€™s predictions fit the actual data.  
In linear regression, the cost function is:

<img src="https://latex.codecogs.com/png.latex?J(\theta)%20=%20\frac{1}{2m}%20\sum_{i=1}^{m}%20\big(%20h_\theta(x^{(i)})%20-%20y^{(i)}%20\big)^2" />

- **m** â†’ number of training examples  
- **hÎ¸(x)** â†’ predicted value (hypothesis)  
- **y** â†’ actual value  
- **Goal** â†’ minimize the cost function (reach the global minimum)

---

## ğŸ”´ Underfitting
- **Definition**: Model is too simple and fails to capture the underlying trend.
- **Accuracy Pattern**:  
  - Training Accuracy: Low  
  - Test Accuracy: Low  
- **Example**: Fitting a straight line to quadratic data.

---

## ğŸ”µ Overfitting
- **Definition**: Model memorizes noise along with the true pattern.
- **Accuracy Pattern**:  
  - Training Accuracy: Very High (close to 100%)  
  - Test Accuracy: Much Lower (large gap)  
- **Example**: High-degree polynomial perfectly fitting training points but failing on unseen data.

---

## ğŸŸ¢ Well-Generalized Model
- **Definition**: Captures the main patterns and generalizes to unseen data.
- **Accuracy Pattern**:  
  - Training Accuracy: High (not perfect)  
  - Test Accuracy: Close to training (small gap)  
- **Example**: A suitable regression model that explains variance and predicts reliably.

---

## ğŸ“Š Ideal Accuracy Values (General Guidelines)

| Model Type       | Training Accuracy | Test Accuracy | Gap Between Train & Test |
|------------------|------------------|---------------|--------------------------|
| Underfitting     | < 70%            | < 70%         | Small gap (but both low) |
| Overfitting      | 95â€“100%          | 60â€“80%        | Large gap                |
| Generalized      | 80â€“95%           | Close (within 2â€“5%) | Small gap, both high |

---

## ğŸ“˜ Ridge Regression (L2 Regularization)

### ğŸ”¹ Cost Function
<img src="https://latex.codecogs.com/png.latex?J(\theta)%20=%20\frac{1}{2m}%20\sum_{i=1}^{m}%20\big(h_\theta(x^{(i)})%20-%20y^{(i)}\big)^2%20+%20\lambda%20\sum_{j=1}^{n}%20\theta_j^2" />

- \( \lambda \) = regularization parameter (strength of penalty).  
- Shrinks coefficients closer to 0 but never eliminates features.

### ğŸ”¹ Effect
- Reduces variance.  
- Prevents overfitting.  
- Keeps all features but **shrinks their impact**.

---

## ğŸ“˜ Lasso Regression (L1 Regularization)

### ğŸ”¹ Cost Function
<img src="https://latex.codecogs.com/png.latex?J(\theta)%20=%20\frac{1}{2m}%20\sum_{i=1}^{m}%20\big(h_\theta(x^{(i)})%20-%20y^{(i)}\big)^2%20+%20\lambda%20\sum_{j=1}^{n}%20|\theta_j|" />

- \( \lambda \) = regularization parameter.  
- Larger \( \lambda \) â†’ stronger shrinkage, more features eliminated.

### ğŸ”¹ Effect
- Prevents overfitting.  
- Creates sparse models (many coefficients = 0).  
- Automatically selects the most important features.

---

## ğŸ“˜ Bias and Variance

### ğŸ”¹ Bias
- **Definition**: Error from simplifying assumptions.  
- **High Bias â†’ Underfitting**  
- Model cannot learn patterns â†’ poor performance on both train & test.  

### ğŸ”¹ Variance
- **Definition**: Error from excessive sensitivity to training data.  
- **High Variance â†’ Overfitting**  
- Model learns noise â†’ high train accuracy, low test accuracy.  

### âš–ï¸ Biasâ€“Variance Tradeoff
- Goal: balance bias and variance.  
- Low Bias + Low Variance = well-generalized model.  

---

## ğŸ›  Role of Regularization
- **Ridge (L2)**: Shrinks coefficients, reduces variance, keeps all features.  
- **Lasso (L1)**: Shrinks some coefficients to zero, reduces variance, performs feature selection.  
- **Elastic Net**: Combines Ridge + Lasso.  

---

## âœ… Final Summary
- **Underfitting** â†’ Low train, low test (High Bias).  
- **Overfitting** â†’ High train, low test (High Variance).  
- **Generalized** â†’ High train, high test, small gap (Balanced).  
- **Ridge (L2)** â†’ Controls variance by shrinking coefficients.  
- **Lasso (L1)** â†’ Controls variance + selects features.  
- **Biasâ€“Variance Tradeoff** â†’ Central idea for building robust models.  
