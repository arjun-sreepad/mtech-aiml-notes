# ğŸ“˜ Regression & Classification in Machine Learning

This note covers:
1. Linear Regression & Cost Function  
2. Underfitting, Overfitting, Generalization  
3. Ridge (L2) and Lasso (L1) Regularization  
4. Biasâ€“Variance Tradeoff  
5. Elastic Net (L1 + L2)  
6. Logistic Regression (Classification)  

---

## âš™ï¸ Linear Regression & Cost Function (MSE)

Linear Regression predicts a **continuous value**.  

**Cost Function (Mean Squared Error - MSE):**  
<img src="https://latex.codecogs.com/png.latex?J(\theta)%20=%20\frac{1}{2m}%20\sum_{i=1}^{m}%20\big(%20h_\theta(x^{(i)})%20-%20y^{(i)}%20\big)^2" />

- **m** â†’ number of training examples  
- **hÎ¸(x)** â†’ predicted value  
- **y** â†’ actual value  
- Goal â†’ minimize cost to find best-fit line  

---

## ğŸ”´ Underfitting
- **Concept:** Model too simple â†’ fails to capture trend.  
- **Train Accuracy:** Low  
- **Test Accuracy:** Low  
- **Example:** Straight line for quadratic data.  

---

## ğŸ”µ Overfitting
- **Concept:** Model too complex â†’ memorizes noise.  
- **Train Accuracy:** Very High (~100%)  
- **Test Accuracy:** Much Lower  
- **Example:** Very high-degree polynomial passing exactly through all points.  

---

## ğŸŸ¢ Generalized Model
- **Concept:** Balanced â†’ captures real patterns, not noise.  
- **Train Accuracy:** High (80â€“95%)  
- **Test Accuracy:** Close to train (small gap).  
- **Example:** Reasonable linear/polynomial model explaining variance and predicting well.  

---

## ğŸ“Š Accuracy Expectations

| Model Type    | Training Accuracy | Test Accuracy | Gap |
|---------------|------------------|---------------|-----|
| Underfitting  | < 70%            | < 70%         | Small but both low |
| Overfitting   | 95â€“100%          | 60â€“80%        | Large |
| Generalized   | 80â€“95%           | ~Close        | Small |

---

## ğŸ“˜ Ridge Regression (L2 Regularization)

Adds **penalty for large coefficients** â†’ prevents overfitting.  

**Cost Function:**  
<img src="https://latex.codecogs.com/png.latex?J(\theta)%20=%20\frac{1}{2m}%20\sum_{i=1}^{m}%20\big(h_\theta(x^{(i)})-y^{(i)}\big)^2%20+%20\lambda%20\sum_{j=1}^{n}\theta_j^2" />

- Shrinks coefficients smoothly  
- Never makes them exactly zero  
- Keeps all features  

**Example:** House price prediction with correlated features (size & bedrooms) â†’ both coefficients reduced but retained.  

---

## ğŸ“˜ Lasso Regression (L1 Regularization)

Adds **absolute value penalty** â†’ some coefficients shrink to zero.  

**Cost Function:**  
<img src="https://latex.codecogs.com/png.latex?J(\theta)%20=%20\frac{1}{2m}%20\sum_{i=1}^{m}%20\big(h_\theta(x^{(i)})-y^{(i)}\big)^2%20+%20\lambda%20\sum_{j=1}^{n}\lvert\theta_j\rvert" />

- Feature selection built-in  
- Sparse models (irrelevant features dropped)  

**Example:** Student performance with many features (attendance, study hours, diet, hobbies) â†’ irrelevant ones (like â€œpets ownedâ€) drop to 0.  

---

## ğŸ“˜ Biasâ€“Variance Tradeoff

- **Bias:** Error from being too simple â†’ Underfitting  
- **Variance:** Error from being too complex â†’ Overfitting  
- **Goal:** Balance both for best generalization  

**Regularization helps balance biasâ€“variance**  
- Ridge â†’ lowers variance (smooth shrinkage)  
- Lasso â†’ lowers variance + does feature selection  

---

## ğŸ§¶ Elastic Net (L1 + L2)

**Idea:** Combine Ridge + Lasso.  

**Cost Function:**  
<img src="https://latex.codecogs.com/png.latex?J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}+\lambda\!\left[\frac{1-\alpha}{2}\sum_{j=1}^{n}\theta_{j}^{2}+\alpha\sum_{j=1}^{n}\lvert\theta_{j}\rvert\right]" />

- **Î± = 0 â†’ Ridge**  
- **Î± = 1 â†’ Lasso**  
- **0 < Î± < 1 â†’ Elastic Net**  

**When to use:**  
- Many correlated features  
- Need shrinkage + feature selection together  

**Example:** Predicting medical outcomes with many tests (blood sugar, cholesterol, blood pressure, etc.) â†’ Elastic Net handles correlation while dropping useless tests.  

---

## ğŸ§­ Logistic Regression (Binary Classification)

Predicts **probability of belonging to a class (0/1)**.  

### 1) Model  
Linear score:  
<img src="https://latex.codecogs.com/png.latex?z=\theta^{\top}x" />

Sigmoid:  
<img src="https://latex.codecogs.com/png.latex?\hat{p}=\sigma(z)=\frac{1}{1+e^{-z}}" />

- Predicts **probability** (0 â‰¤ p â‰¤ 1).  
- Classify using threshold (default 0.5).  

---

### 2) Example
- **Email Spam:**  
  - Email A â†’ 0.9 â†’ Spam  
  - Email B â†’ 0.2 â†’ Not spam  
- **Student Pass/Fail:**  
  - Hours studied = 10 â†’ p=0.8 â†’ Pass  
  - Hours studied = 2 â†’ p=0.3 â†’ Fail  

---

### 3) Loss Function (Log-Loss / Cross-Entropy)  
<img src="https://latex.codecogs.com/png.latex?J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\big[y^{(i)}\log\hat{p}^{(i)}+(1-y^{(i)})\log(1-\hat{p}^{(i)})\big]" />

- Punishes wrong confident predictions heavily  
- Convex â†’ easier optimization  

---

### 4) Regularized Logistic Regression
- Add **L2** penalty â†’ shrink all weights  
- Add **L1** penalty â†’ drop irrelevant features  
- Use **Elastic Net** for mixed benefit  

---

### 5) Evaluation Metrics
- **Confusion Matrix:** TP, FP, TN, FN  
- **Accuracy:** (TP+TN)/(Total)  
- **Precision:** TP/(TP+FP)  
- **Recall:** TP/(TP+FN)  
- **F1:** harmonic mean of Precision & Recall  
- **ROC-AUC/PR-AUC:** quality independent of threshold  

---

## ğŸ§° Step-by-Step Workflow

1. **Collect Data** (e.g., Students â†’ Pass/Fail)  
2. **Preprocess Features** (scale numbers, encode categories)  
3. **Split Data** (Train/Validation/Test)  
4. **Choose Model** (Logistic, add Ridge/Lasso/Elastic Net if needed)  
5. **Train** (minimize log-loss via gradient descent)  
6. **Tune Threshold** (default 0.5; adjust if recall/precision more important)  
7. **Evaluate** (metrics, confusion matrix)  
8. **Interpret** (coefficients â†’ impact on odds)  
9. **Deploy & Monitor** (track drift, recalibrate thresholds)  

---

## âœ… Final Summary

- **Linear Regression** â†’ continuous prediction  
- **Logistic Regression** â†’ probability/class prediction  
- **Underfitting** â†’ low train & test (high bias)  
- **Overfitting** â†’ high train, low test (high variance)  
- **Ridge** â†’ shrink all weights (L2)  
- **Lasso** â†’ shrink some weights to zero (L1)  
- **Elastic Net** â†’ combo (L1+L2)  
- **Biasâ€“Variance Tradeoff** â†’ main principle for generalization  
- **Logistic Regression Metrics** â†’ accuracy, precision, recall, F1, AUC  

