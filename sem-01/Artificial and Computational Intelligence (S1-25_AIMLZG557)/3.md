# üìò Session 6 ‚Äì Heuristic Design & Local Search

---

## Heuristics in AI  

A heuristic in Artificial Intelligence is an informed guess that helps guide search algorithms toward the goal more efficiently. 
Unlike uninformed searches such as BFS or DFS, which blindly explore nodes, heuristics use problem-specific knowledge to prioritize 
which states should be expanded first. This makes search much faster and more practical for complex problems.  

**Example**: In GPS navigation, rather than exploring every possible road, the system uses straight-line distance as a heuristic 
to estimate which route looks most promising.  

**Advantages**: Heuristics reduce search time, improve efficiency, and make otherwise infeasible problems solvable.  
**Disadvantages**: They can be costly to compute, and a poorly designed heuristic may mislead the search.  

---

## Designing Good Heuristics  

A good heuristic should be:  
- **Admissible** ‚Äì it never overestimates the true cost to the goal.  
- **Consistent** ‚Äì it satisfies the triangle inequality:  
  h(n) ‚â§ c(n, a, n‚Ä≤) + h(n‚Ä≤).  
- **Informative** ‚Äì it should closely approximate the real cost.  
- **Efficient** ‚Äì it should be quick to compute.  

**Methods to design heuristics**:  
- By considering **relaxed problems**, which are simplified versions of the original problem (e.g., ignoring constraints).  
- By solving **subproblems** and using their results as heuristics.  
- By using **learning-based models**, which approximate cost with weighted features, such as h(n) = c‚ÇÅx‚ÇÅ + c‚ÇÇx‚ÇÇ + ‚Ä¶  

---

## Common Heuristic Functions  

### Euclidean Distance  
This heuristic calculates the straight-line distance between two points.  
Formula:  
h(n) = ‚àö((x‚ÇÅ - x‚ÇÇ)¬≤ + (y‚ÇÅ - y‚ÇÇ)¬≤)  

**Example**: GPS navigation where a car drives toward a destination.  
**Advantages**: Accurate in continuous spaces; simple to understand.  
**Disadvantages**: Unrealistic in grid environments (e.g., roads) because cars cannot always move diagonally.  

### Manhattan Distance  
This heuristic calculates distance assuming only horizontal or vertical movement.  
Formula:  
h(n) = |x‚ÇÅ - x‚ÇÇ| + |y‚ÇÅ - y‚ÇÇ|  

**Example**: The 8-puzzle problem or robots navigating in warehouses.  
**Advantages**: Matches environments where movement is restricted to four directions.  
**Disadvantages**: Underestimates cost if diagonal movement is possible.  

### Misplaced Tiles Heuristic  
This heuristic simply counts how many tiles are not in their correct positions.  

**Example**: In the 8-puzzle, if 5 tiles are misplaced, then h = 5.  
**Advantages**: Extremely simple and fast to compute.  
**Disadvantages**: Less accurate than other heuristics; may result in exploring more nodes.  

---

## Effective Branching Factor  

The effective branching factor measures how efficiently a heuristic reduces the number of nodes expanded compared to blind search.  

Formula:  
N + 1 ‚âà 1 + b* + (b*)¬≤ + ‚Ä¶ + (b*)^d  

where N = number of nodes expanded, d = solution depth, and b* = effective branching factor.  

**Example**: If BFS expands 1000 nodes but A* expands only 50 nodes, A* is using a much smaller effective branching factor.  
**Advantages**: Provides a clear metric of heuristic performance.  
**Disadvantages**: Requires experimental measurement and comparison.  

---

## Local Search and Optimization  

Local search works with a single current state rather than building an entire path. It uses an objective or fitness function 
to measure how good a state is and is suitable for optimization problems with very large state spaces.  

---

### Hill Climbing  

**Concept**: Hill climbing is a greedy search method that moves from the current state to a better neighboring state.  
It continues until no improvement is possible.  

**Example**: The N-Queens problem, where queens are moved to minimize conflicts.  
**Advantages**: Simple to implement, requires little memory.  
**Disadvantages**: Can get stuck in local maxima, plateaus, or ridges, and may fail to find the global optimum.  

Variants include Simple Hill Climbing, Steepest-Ascent Hill Climbing, Stochastic Hill Climbing, and Random Restart Hill Climbing.  

---

### Simulated Annealing  

**Concept**: Inspired by metallurgy, simulated annealing occasionally accepts worse moves to escape local maxima.  
The probability of accepting worse moves decreases over time as the ‚Äútemperature‚Äù cools.  

**Example**: Circuit design optimization.  
**Advantages**: Escapes local maxima; effective in large, rough landscapes.  
**Disadvantages**: Requires careful tuning of the cooling schedule; convergence can be slow.  

---

### Local Beam Search  

**Concept**: Maintains k states at a time, expands all successors, and keeps the best k for the next step.  
A stochastic variant selects successors probabilistically based on their fitness.  

**Example**: Text prediction, where multiple completions are considered and refined.  
**Advantages**: Explores multiple regions simultaneously, more robust than hill climbing.  
**Disadvantages**: May still converge to local maxima, depends heavily on choice of k.  

---

### Genetic Algorithms (GA)  

**Concept**: Genetic algorithms are inspired by evolution. A population of solutions evolves through selection, crossover, and mutation.  

**Example**: Job scheduling, route planning, and optimization problems.  
**Advantages**: Excellent at exploring huge, complex search spaces; can produce innovative solutions.  
**Disadvantages**: Computationally expensive and may converge slowly if not tuned well.  

---

## ‚úÖ Summary  

- Heuristics guide search by estimating cost to the goal.  
- Good heuristics are admissible, consistent, and efficient.  
- Common heuristics: Euclidean, Manhattan, Misplaced Tiles.  
- Effective branching factor measures heuristic performance.  
- Local search algorithms optimize using single states:  
  - Hill Climbing (simple but stuck in local maxima).  
  - Simulated Annealing (escapes local maxima probabilistically).  
  - Local Beam Search (parallel exploration).  
  - Genetic Algorithms (evolutionary optimization).  

---
